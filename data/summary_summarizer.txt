 Machine Learning for Computational Linguistics Lecture 1: Introduc.on to the course & Linear Algebra Srikanth Madikeri, 20.09.2023 course organization Tutors Fei Gao Yuliia Frund Tutorat every Tuesday between 12h15 and 13h45 (AN D-3-02/06) Por Folio: 25% exercises, 75% final exam, 4 exercises in total. WARNIN G: Final exam is on 20th December, not 17th December.

 We use bold small leners for vectors, regular font for scalars is transpose opera Pon (next slide) T is a scalar b Dot product example. Two vectors are orthogonal if their dot product is 0Vectors (contd.) Vectors are sequences of scalars dimensions of a vector = number of vector elements. A vector can be decomposed into two components: Magnitude: length of the vector. Direc Pon is to write vector as a column a.k.a column vector.

 A set of vectors are linearly dependent if they can be scaled and added to obtain 0.0. The rank of the matrix is the size of the largest set of linearly independent vectors 1 2 3 X = 0 2 2 1 4 5. The resul Png matrix is X X m b 1 2 2 4 2 4 3 2 3 2 X = X X X. When a matrix is mul Pplied with only if X X a = n 1 2, we get back column vector x x x X = 21 22 23 x x. When m=1 we get row vector 31 32 33.

 Py Torch uses tensors as a fundamental representa Pon of data. For more fun with matrix idenP Pes see Matrix cookbook Matrix Cookbook. Only square matrices can be inverted Only full-rank matrices are allowed to be inverted. (A B) = B A for more fun, see Matrix Cookbooks for more of the fun with Matrix IdenP Ps. (A B: A 1 A A = A A A = I) (A B = A B A) (A: A A = A A A = A)