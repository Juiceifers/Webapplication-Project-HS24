<PAGE 0>
Srikanth Madikeri, 20.09.2023
	Machine Learning for 
	Computational Linguistics
	Lecture 1: Introduction to the course & Linear Algebra
<PAGE END>
<PAGE 1>
Course organization
	Tutors
	Fei Gao
	Yuliia Frund
	Tutorat every Tuesday between 12h15 and 13h45 (AND-3-02/06)
<PAGE END>
<PAGE 2>
Course organization
	Evaluation
	• Portfolio: 25% exercises, 75% final exam 
	• 4 exercises in total. Tentative release dates 
	• 4th October, 25th October, 8th November, 29th November  
	• 2 weeks to submit 
	• OK to do in groups of 2, but must be declared 
	• WARNING: Final exam is on 20th December, not 17th December
<PAGE END>
<PAGE 3>
What is Machine Learning?
	Keywords: statistical algorithms, data, artificial neural networks
<PAGE END>
<PAGE 4>
Example: Machine Translation
	Google Translate
	• An example of a task with a simple definition 
	• Humans find it easy to define what they want to do 
	• Hard to automate 
	• Improvements in underlying technology: Rule based ==> Statistical machine learning ==> Deep 
	Neural networks 
	• Rule-based: You have to come up with all possible rules (and exceptions)! 
	• E.g. French has too many exceptions 
	• Statistical machine learning: Need to choose a different model for different task 
	• Deep Neural Networks: Recent approaches converge to a unified model for many tasks
<PAGE END>
<PAGE 5>
Object segmentation
<PAGE END>
<PAGE 6>
Popular examples
	• ChatGPT: Large Language Models with reinforcement learning 
	• Autopilot in cars 
	• Voice assistants in your phones
<PAGE END>
<PAGE 7>
The Learning problem
	• Define the task. E.g. Translate text from English to French 
	• Training set: consists of data (X) and targets (Y) 
	• E.g. Machine Translation (En -> Fr) How are you —> Ça va? 
	• X: How are you? 
	• Y: Ça va? 
	• Problem: Learn a mapping that transforms X to Y 
	• Learning: Learn the map from many examples of (X, Y) 
	• Inference: Given X, predict Y
<PAGE END>
<PAGE 8>
Approaching the learning problem
	• We are humans. So, how do we solve the problem? 
	• Formulate it in a way that we can convert it into a computer program 
	• Check if a feasible solution exists
<PAGE END>
<PAGE 9>
Neurons in the Human Brain
	• Brain contains billions of 
	neuron cells 
	• They are interconnected 
	• Each neuron has many 
	incoming connections and 
	outgoing connections 
	• Myelination: pathways get 
	stronger the more they are 
	used
	A multipolar neuron (image from Wikipedia)
<PAGE END>
<PAGE 10>
Spiking neurons
	• (Over-)Simplified image of the neuron 
	• Viewed as a processing unit 
	• Information flows when it is active 
	• Spike of voltage for a brief moment 
	• Spike implies activity 
	• Check chapter 1 from Mehlig’s textbook for a nice 
	introduction
	Image from Mehlig 2019
<PAGE END>
<PAGE 11>
A computation unit based on the neuron
	Thresholding function
	Source: Haykin 3e
	Relevant material added to OLAT
<PAGE END>
<PAGE 12>
AI revolution
	• The foundations for the methods we use have already existed for decades 
	• Mcculoh and Pitts neuron in the 60s 
	• Perceptron from the 60s 
	• Multilayer Perceptron (MLP) from the 80s 
	• What’s new? 
	• More data: internet has become a great source of data 
	• Better hardware: GPUs! 
	• Democratization of data and code: Github, Huggingface
<PAGE END>
<PAGE 13>
CPUs vs GPUs
	How CPU and GPU exist in the same system
	CPU: Central Processing Unit
	GPU: Graphic Processing Unit
	GPUs are highly efficient at executing parallel 
	code
	Poor choice to execute serial code
	How consumer grade GPU looks physically
<PAGE END>
<PAGE 14>
Topics covered in the course
	• We will cover only deep learning techniques (course title is a misnomer) 
	• Foundations of deep learning 
	• Model architectures: Transformers, RNNs, LSTMs, Encoder-decoder 
	• LLM, Instruction fine-tuning 
	• Adapters
<PAGE END>
<PAGE 15>
What will not be covered
	• Reinforcement learning 
	• CNNs (in-depth) 
	• Probabilistic models (e.g. GMMs, HMMs, Variational learning)
<PAGE END>
<PAGE 16>
Textbooks and references
	• For PyTorch programming better to follow online videos and Github repositories 
	• Main reference: Speech and Language processing by Jurafsky and Martin (3rd 
	edition) 
	• https://web.stanford.edu/~jurafsky/slp3/ 
	• Other resources: 
	• D2l.ai 
	• Deep learning https://www.deeplearningbook.org/
<PAGE END>
<PAGE 17>
Course Evaluation
	• 25% exercises 
	• OK to do in groups of 2, but must be declared 
	• Source of materials should be declared (e.g. slide number, text book section, 
	ChatGPT, educated guess) 
	• Tutorials on Tuesday 12h15 — 13h30 (AND-3-02/06) 
	• 75% final exam (20th December NOT 17th December) 
	• Digital exam
<PAGE END>
<PAGE 18>
Linear Algebra
	Recap
	• Algebra of lines 
	• What is a line 
	• In 1 dimension: 
	 
	•
	: slope 
	•
	 intercept 
	• This is a 1 dimensional line. In 2 dimension we get a plane 
	• In n-dimensions we have a hyperplane
	y = mx + c
	m
	c :
	x
	y
	y = mx + c
	c
	θ
	m = tan θ
<PAGE END>
<PAGE 19>
Hyperplanes
	• Equation of hyperplane: 
	 
	• Where 
	 are vectors in 
	dimensions. This is also stated as 
	 
	• We use bold small letters for vectors, regular font for scalars 
	•
	 is transpose operation (next slide) 
	•
	 is a scalar 
	wTx + b = 0
	w, x
	n−
	x ∈ℝn
	T
	b
	wTx + b = 0
	wTx + b > 0
	wTx + b < 0
<PAGE END>
<PAGE 20>
Dot product example
	• Let us look at a 5-dimensional example
	x =
	1.0
	2.0
	3.0
	4.0
	5.0
	w =
	10.0
	5.0
	0.0
	−2.0
	−2.0
	wT = [10.0 5.0
	0.0
	−2.0
	−2.0]
	x =
	1.0
	2.0
	3.0
	4.0
	5.0
	wTx = 10.0 × 1.0 + 2.0 × 5.0 + 3.0 × 0.0 + …
	wTx = 10.0
	+ 10.0
	+ 0.0
	+ …
	wTx = −1
<PAGE END>
<PAGE 21>
Geometric interpretation
	• The part  that goes in the direction of 
	 (or vice versa!) i.e. projection of  onto 
	 
	• This is why it can be used to check similarity between two vectors! 
	• Two vectors are orthogonal if their dot product is 0
	x
	w
	x
	w
	w
	x
	|x|
	wTx = |w||x|cos θ
<PAGE END>
<PAGE 22>
Vectors (contd.)
	• Vectors are sequences of scalars 
	• dimensions of a vector = number of vector elements 
	• Convention is to write vector as a column a.k.a column vector
	w =
	w1
	w2
	w3
	w4
	w5
	wT = [w1
	w2
	w3 w4
	w5]
	x =
	x1
	x2
	x3
	x4
	x5
	wTx =
	w1
	w2
	w3
	w4
	w5
	×
	×
	×
	×
	×
	x1
	x2
	x3
	x4
	x5
	dim(w) = 5
	dim(wTx) = 1
	wTx = w1x1 + w2x2 + w3x3 + w4x4 + w5x5
<PAGE END>
<PAGE 23>
Magnitude and direction of vectors
	• A vector can be decomposed into two components 
	• Magnitude: length of the vector 
	• Direction of the vector. It has magnitude 1. 
	• Obtained by dividing the vector by its magnitude: a.k.a length normalization
	w =
	w1
	w2
	w3
	w4
	w5
	|w| =
	w2
	1 + w2
	2 + …w2
	5
	̂w =
	w1/|w|
	w2/|w|
	w3/|w|
	w4/|w|
	w5/|w|
	x
	y
	w
	̂w
<PAGE END>
<PAGE 24>
Special vectors
	0 =
	0
	0
	0
	⋮
	1 =
	1
	1
	1
	⋮
	ei =
	0
	0
	0
	⋮
	1
	⋮0
	Zero vector
	One vector
	Standard basis vector
	Foundation for word embeddings
	e2 =
	0
	1
	0
	⋮
	0
	e1 =
	1
	0
	0
	⋮
	0
<PAGE END>
<PAGE 25>
Vector operations
	x =
	1.0
	2.0
	3.0
	4.0
	5.0
	α x =
	α1.0
	α2.0
	α3.0
	α4.0
	α5.0
	α is a scalar
	y =
	1.5
	2.2
	3.3
	2.0
	−5.0
	x + y =
	1.0 + 1.5
	2.0 + 2.2
	3.0 + 3.3
	4.0 + 2.0
	5.0 −5.0
	=
	2.5
	4.2
	6.3
	6.0
	0.0
	10x =
	10.0
	20.0
	30.0
	40.0
	50.0
	Scalar multiplication
	Addition of two vectors 
	is element-wise addition
<PAGE END>
<PAGE 26>
Matrices
	C
	• Two-dimensional arrangement of values 
	• First dimension is row, number of rows denoted by 
	 
	• Second dimension is column, number of columns denoted by  
	• We use capital bold letters to denote matrices 
	• Sometimes it is helpful to subscript the shape of the matrix
	m
	n
	• When n=1, we get back column vector 
	• When m=1, we get row vector
	 matrix
	3 × 3
	X ∈ℝ3,3
	Generally we say X ∈ℝm,n
	X =
	0.8
	0.7
	0.4
	0.9
	0.1
	0.3
	0.2
	0.7
	0.4
	Columns
	Rows
	X =
	x11
	x12
	x13
	x21 x22
	x23
	x31
	x32
	x33
	Matrix with scalar variables
<PAGE END>
<PAGE 27>
Matrix shapes
	0.8 0.7
	0.4
	0.9 0.1
	0.3
	0.2 0.7
	0.4
	0.8 0.7
	0.4
	−0.2
	−0.1
	0.9 0.1
	0.3
	−0.6
	0.6
	0.2 0.7
	0.4
	0.9
	0.2
	[
	]
	Tall & skinny 
	m ≫n
	Short & fat m ≪n
	Square m = n
	Rectangular m ≠n
<PAGE END>
<PAGE 28>
Matrix Operations
	Addition of two matrices
	0.8
	0.7 0.4
	0.9
	0.1 0.3
	0.2
	0.7 0.4
	0.2
	−0.7
	0.6
	0.1
	0.1
	0.3
	−0.2
	0.3
	0.4
	+
	=
	0.8 + 0.2 0.7 −0.7 0.4 + 0.6
	0.9 + 0.1 0.1 + 0.1 0.3 + 0.3
	0.2 −0.2 0.7 + 0.3 0.4 + 0.4
	=
	1.0
	0.0
	1.0
	1.0
	0.2
	0.6
	0.0
	1.0
	0.8
	Both matrices need to be of same dimensions
<PAGE END>
<PAGE 29>
Matrix operations
	Scalar multiplication
	2 ×
	0.8
	−0.7
	0.4
	−0.9
	0.1
	0.3
	0.2
	0.7
	0.4
	=
	1.6
	−1.4
	0.8
	−1.8
	0.2
	0.6
	0.4
	1.4
	0.8
<PAGE END>
<PAGE 30>
Matrix operations
	Trace and Transpose
	0.8 0.7
	0.4
	0.9 0.1
	0.3
	0.2 0.7
	0.4
	0.8
	0.7
	0.4
	−0.2
	−0.1
	0.9
	0.1
	0.3
	−0.6
	0.6
	0.2
	0.7
	0.4
	0.9
	0.2
	Trace
	Sum of diagonal elements
	0.8 + 0.1 + 0.4
	0.8 + 0.1 + 0.4
	tr(A)
	Transpose AT
	Swap rows and columns
	0.8
	0.9
	0.2
	0.7
	0.1
	0.7
	0.4
	0.3
	0.4
	0.8
	0.9
	0.2
	0.7
	0.1
	0.7
	0.4
	0.3
	0.4
	⋮
	⋮
	⋮
	Symmetric matrix: A = AT
	0.8 0.9
	0.2
	0.9 0.1
	0.7
	0.2 0.7
	0.4
<PAGE END>
<PAGE 31>
Linear independence
	A set of vectors are linearly dependent if they can be scaled and added to obtain 
	0. 
	Otherwise, they are linearly independent  
	( 1,0,0 ) , ( 0,1,0 ) and ( 0,0,1 ) linearly independent. 
	( 1,0,1 ) , ( 2,-1,1 ) , ( 3,-1,2 ) are linearly dependent.
<PAGE END>
<PAGE 32>
Matrix rank
	• Consider the row (or column) vectors in a matrix 
	• The rank of the matrix is the size of the largest set of linearly independent 
	vectors
	X =
	1
	2
	3
	0
	2
	2
	1
	4
	5
	• Rank of the above matrix is 2 
	• Only row 1 and 2 are linearly independent
<PAGE END>
<PAGE 33>
Matrix multiplication 
	• Let 
	 be a 
	 matrix 
	• Let 
	 be 
	 matrix 
	•
	 can be multiplied with 
	 only if 
	 
	• The resulting matrix 
	 is 
	X1
	m × n
	X2
	a × b
	X1
	X2
	a = n
	X1X2
	m × b
	X1 = [
	2 4
	3 2
	1 2]
	X2 = [
	3
	2
	1
	1]
	X1X2 = [
	2
	4
	3
	2
	1
	2] [
	3
	2
	1
	1] =
<PAGE END>
<PAGE 34>
Special matrices
	D = [
	0.8 0.0
	0.0
	0.0 0.1
	0.0
	0.0 0.0
	0.4]
	I3 = [
	1
	0 0
	0
	1 0
	0
	0 1]
	In =
	1
	0
	0
	…
	0
	0
	1
	0
	…
	0
	0
	0
	1
	…
	0
	⋮
	⋮
	⋮
	…
	0
	0
	⋮
	⋮
	…
	?
	Diagonal matrix
	Identity matrix
	 identity matrix
	3 × 3
	 identity matrix
	n × n
	Any matrix multiplied with  is the matrix itself
	I
	AI = IA = A
	When a matrix is pre-
	multiplied by a diagonal 
	matrix, i.e 
	 it scales 
	the rows
	DX
	When it is post-
	multiplied, that is 
	 it 
	scales the columns
	XD
<PAGE END>
<PAGE 35>
Matrix identities
	• Distributive law 
	 
	• Associative law 
	 
	• Transpose after product: 
	A(B + C) = AB + AC
	A + (B + C) = (A + B) + C
	(AB)T = BTAT
	For more fun with matrix identities see “Matrix cookbook”
<PAGE END>
<PAGE 36>
Matrix inverse
	• Only square matrices can be inverted 
	• Only full-rank matrices can be inverted
	AA−1 = A−1A = I
<PAGE END>
<PAGE 37>
Tensors
	A stack of matrices
	• Multidimensional arrangement of numbers
	• PyTorch uses tensors as a fundamental representation of data
<PAGE END>
<PAGE 38>
Linear algebra topics covered in the next class
	• Geometric Matrix-vector product 
	• Eigen values  
	• Singular Value Decomposition
<PAGE END>
