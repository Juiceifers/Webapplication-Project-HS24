
    <html>
    <head>
    </head>
    <body>
      <div>
          <h2>Tree:</h2>
            <p>
            
                
                    .<br>
                
                    ├─■──repositories_perceptron_pytorch_chatgpt_jurafsky ── Topic: 0<br>
                
                    └─■──x1x2_w1x1_w2x2_w4x4_w3x3 ── Topic: 1<br>
                
                    <br>
                
            
            </p>
      </div>

      <div>
        <h2>Index:</h2>

        <h3>Summaries</h3>
        
            <a href="#sum_lecture1-introduction.pdf">lecture1-introduction.pdf summary</a><br>
        

        <br>

        <h3>Topics and Docs</h3>
        
        <a href="#t_0">TOPIC 0: 	repositori, perceptron, pytorch</a><br>
            
                &emsp;<a href="#d_0">lecture1-introduction.pdf /page_0, doc-id 0</a><br>
            
                &emsp;<a href="#d_1">lecture1-introduction.pdf /page_1, doc-id 1</a><br>
            
                &emsp;<a href="#d_2">lecture1-introduction.pdf /page_2, doc-id 2</a><br>
            
                &emsp;<a href="#d_3">lecture1-introduction.pdf /page_3, doc-id 3</a><br>
            
                &emsp;<a href="#d_4">lecture1-introduction.pdf /page_4, doc-id 4</a><br>
            
                &emsp;<a href="#d_5">lecture1-introduction.pdf /page_5, doc-id 5</a><br>
            
                &emsp;<a href="#d_6">lecture1-introduction.pdf /page_6, doc-id 6</a><br>
            
                &emsp;<a href="#d_7">lecture1-introduction.pdf /page_7, doc-id 7</a><br>
            
                &emsp;<a href="#d_8">lecture1-introduction.pdf /page_8, doc-id 8</a><br>
            
                &emsp;<a href="#d_9">lecture1-introduction.pdf /page_9, doc-id 9</a><br>
            
                &emsp;<a href="#d_10">lecture1-introduction.pdf /page_10, doc-id 10</a><br>
            
                &emsp;<a href="#d_11">lecture1-introduction.pdf /page_11, doc-id 11</a><br>
            
                &emsp;<a href="#d_12">lecture1-introduction.pdf /page_12, doc-id 12</a><br>
            
                &emsp;<a href="#d_13">lecture1-introduction.pdf /page_13, doc-id 13</a><br>
            
                &emsp;<a href="#d_14">lecture1-introduction.pdf /page_14, doc-id 14</a><br>
            
                &emsp;<a href="#d_15">lecture1-introduction.pdf /page_15, doc-id 15</a><br>
            
                &emsp;<a href="#d_16">lecture1-introduction.pdf /page_16, doc-id 16</a><br>
            
                &emsp;<a href="#d_17">lecture1-introduction.pdf /page_17, doc-id 17</a><br>
            
                &emsp;<a href="#d_37">lecture1-introduction.pdf /page_37, doc-id 37</a><br>
            
                &emsp;<a href="#d_38">lecture1-introduction.pdf /page_38, doc-id 38</a><br>
            
        
        <a href="#t_1">TOPIC 1: 	x1x2, w1x1, w2x2</a><br>
            
                &emsp;<a href="#d_18">lecture1-introduction.pdf /page_18, doc-id 18</a><br>
            
                &emsp;<a href="#d_19">lecture1-introduction.pdf /page_19, doc-id 19</a><br>
            
                &emsp;<a href="#d_20">lecture1-introduction.pdf /page_20, doc-id 20</a><br>
            
                &emsp;<a href="#d_21">lecture1-introduction.pdf /page_21, doc-id 21</a><br>
            
                &emsp;<a href="#d_22">lecture1-introduction.pdf /page_22, doc-id 22</a><br>
            
                &emsp;<a href="#d_23">lecture1-introduction.pdf /page_23, doc-id 23</a><br>
            
                &emsp;<a href="#d_24">lecture1-introduction.pdf /page_24, doc-id 24</a><br>
            
                &emsp;<a href="#d_25">lecture1-introduction.pdf /page_25, doc-id 25</a><br>
            
                &emsp;<a href="#d_26">lecture1-introduction.pdf /page_26, doc-id 26</a><br>
            
                &emsp;<a href="#d_27">lecture1-introduction.pdf /page_27, doc-id 27</a><br>
            
                &emsp;<a href="#d_28">lecture1-introduction.pdf /page_28, doc-id 28</a><br>
            
                &emsp;<a href="#d_29">lecture1-introduction.pdf /page_29, doc-id 29</a><br>
            
                &emsp;<a href="#d_30">lecture1-introduction.pdf /page_30, doc-id 30</a><br>
            
                &emsp;<a href="#d_31">lecture1-introduction.pdf /page_31, doc-id 31</a><br>
            
                &emsp;<a href="#d_32">lecture1-introduction.pdf /page_32, doc-id 32</a><br>
            
                &emsp;<a href="#d_33">lecture1-introduction.pdf /page_33, doc-id 33</a><br>
            
                &emsp;<a href="#d_34">lecture1-introduction.pdf /page_34, doc-id 34</a><br>
            
                &emsp;<a href="#d_35">lecture1-introduction.pdf /page_35, doc-id 35</a><br>
            
                &emsp;<a href="#d_36">lecture1-introduction.pdf /page_36, doc-id 36</a><br>
            
        

      </div>

      <div>
        <h2>Summaries:</h2>
        
            <h3 id="sum_lecture1-introduction.pdf">lecture1-introduction.pdf</h3><br>
            
                <p> Srikanth Madikeri, 20.09.2023 Machine Learning for Computational Linguistics Lecture 1: Introduction to the course &amp; Linear Algebra. Final exam is on 20th December, not 17th December. We will cover only deep learning techniques (course title is a misnomer) Foundations of deep learning: Transformers, RN Ns, LST Ms, Encoder-decoder, LL M, Instruction fine-tuning  Adapters. What will not be covered: Reinforcement learning (in-depth)</p>
            
                <p> Vectors are sequences of scalars, dimensions of a vector = number of vector elements. Two vectors are orthogonal if their dot product is 0 x w x w w x |x| w Tx = |w||x|cos. A vector can be decomposed into two components: Magnitude: length of the vector. The part that goes in the direction of (or vice versa!) i.e. projection of onto. This is why it can be used to check similarity between two vectors!</p>
            
                <p> A set of vectors are linearly dependent if they can be scaled and added to obtain 0.4. The rank of the matrix is the size of the largest set of linearly independent vectors X = 1 2 3 0 2 2 1 4 5. The resulting matrix is X1 m  n X2 a  b X1 X2  = n X1X2 m  b. The next class of tensors uses tensors as a representation of data. Py Torchors are a fundamental representation of the data.</p>
            
            <br>
        
      </div>

      <div>
      
        <div>
        <h2 id="t_0">TOPIC 0: 	repositori, perceptron, pytorch</h2>
            
                <div>
                    <h3 id="d_0">lecture1-introduction.pdf /page_0, doc-id 0</h3>
                    <p style="margin:10px">
                        
                            Srikanth Madikeri, 20.09.2023<br>
                        
                            Machine Learning for <br>
                        
                            Computational Linguistics<br>
                        
                            Lecture 1: Introduction to the course &amp; Linear Algebra<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_1">lecture1-introduction.pdf /page_1, doc-id 1</h3>
                    <p style="margin:10px">
                        
                            Course organization<br>
                        
                            Tutors<br>
                        
                            Fei Gao<br>
                        
                            Yuliia Frund<br>
                        
                            Tutorat every Tuesday between 12h15 and 13h45 (AND-3-02/06)<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_2">lecture1-introduction.pdf /page_2, doc-id 2</h3>
                    <p style="margin:10px">
                        
                            Course organization<br>
                        
                            Evaluation<br>
                        
                            • Portfolio: 25% exercises, 75% final exam <br>
                        
                            • 4 exercises in total. Tentative release dates <br>
                        
                            • 4th October, 25th October, 8th November, 29th November  <br>
                        
                            • 2 weeks to submit <br>
                        
                            • OK to do in groups of 2, but must be declared <br>
                        
                            • WARNING: Final exam is on 20th December, not 17th December <br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_3">lecture1-introduction.pdf /page_3, doc-id 3</h3>
                    <p style="margin:10px">
                        
                            What is Machine Learning?<br>
                        
                            Keywords: statistical algorithms, data, artificial neural networks<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_4">lecture1-introduction.pdf /page_4, doc-id 4</h3>
                    <p style="margin:10px">
                        
                            Example: Machine Translation<br>
                        
                            Google Translate<br>
                        
                            • An example of a task with a simple definition <br>
                        
                            • Humans find it easy to define what they want to do <br>
                        
                            • Hard to automate <br>
                        
                            • Improvements in underlying technology: Rule based ==&gt; Statistical machine learning ==&gt; Deep <br>
                        
                            Neural networks <br>
                        
                            • Rule-based: You have to come up with all possible rules (and exceptions)! <br>
                        
                            • E.g. French has too many exceptions <br>
                        
                            • Statistical machine learning: Need to choose a different model for different task <br>
                        
                            • Deep Neural Networks: Recent approaches converge to a unified model for many tasks<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_5">lecture1-introduction.pdf /page_5, doc-id 5</h3>
                    <p style="margin:10px">
                        
                            Object segmentation<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_6">lecture1-introduction.pdf /page_6, doc-id 6</h3>
                    <p style="margin:10px">
                        
                            Popular examples<br>
                        
                            • ChatGPT: Large Language Models with reinforcement learning <br>
                        
                            • Autopilot in cars <br>
                        
                            • Voice assistants in your phones<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_7">lecture1-introduction.pdf /page_7, doc-id 7</h3>
                    <p style="margin:10px">
                        
                            The Learning problem<br>
                        
                            • Define the task. E.g. Translate text from English to French <br>
                        
                            • Training set: consists of data (X) and targets (Y) <br>
                        
                            • E.g. Machine Translation (En -&gt; Fr) How are you —&gt; Ça va? <br>
                        
                            • X: How are you? <br>
                        
                            • Y: Ça va? <br>
                        
                            • Problem: Learn a mapping that transforms X to Y <br>
                        
                            • Learning: Learn the map from many examples of (X, Y) <br>
                        
                            • Inference: Given X, predict Y<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_8">lecture1-introduction.pdf /page_8, doc-id 8</h3>
                    <p style="margin:10px">
                        
                            Approaching the learning problem<br>
                        
                            • We are humans. So, how do we solve the problem? <br>
                        
                            • Formulate it in a way that we can convert it into a computer program <br>
                        
                            • Check if a feasible solution exists<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_9">lecture1-introduction.pdf /page_9, doc-id 9</h3>
                    <p style="margin:10px">
                        
                            Neurons in the Human Brain<br>
                        
                            • Brain contains billions of <br>
                        
                            neuron cells <br>
                        
                            • They are interconnected <br>
                        
                            • Each neuron has many <br>
                        
                            incoming connections and <br>
                        
                            outgoing connections <br>
                        
                            • Myelination: pathways get <br>
                        
                            stronger the more they are <br>
                        
                            used<br>
                        
                            A multipolar neuron (image from Wikipedia)<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_10">lecture1-introduction.pdf /page_10, doc-id 10</h3>
                    <p style="margin:10px">
                        
                            Spiking neurons<br>
                        
                            • (Over-)Simplified image of the neuron <br>
                        
                            • Viewed as a processing unit <br>
                        
                            • Information flows when it is active <br>
                        
                            • Spike of voltage for a brief moment <br>
                        
                            • Spike implies activity <br>
                        
                            • Check chapter 1 from Mehlig’s textbook for a nice <br>
                        
                            introduction<br>
                        
                            Image from Mehlig 2019<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_11">lecture1-introduction.pdf /page_11, doc-id 11</h3>
                    <p style="margin:10px">
                        
                            A computation unit based on the neuron<br>
                        
                            Thresholding function<br>
                        
                            Source: Haykin 3e<br>
                        
                            Relevant material added to OLAT<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_12">lecture1-introduction.pdf /page_12, doc-id 12</h3>
                    <p style="margin:10px">
                        
                            AI revolution<br>
                        
                            • The foundations for the methods we use have already existed for decades <br>
                        
                            • Mcculoh and Pitts neuron in the 60s <br>
                        
                            • Perceptron from the 60s <br>
                        
                            • Multilayer Perceptron (MLP) from the 80s <br>
                        
                            • What’s new? <br>
                        
                            • More data: internet has become a great source of data <br>
                        
                            • Better hardware: GPUs! <br>
                        
                            • Democratization of data and code: Github, Huggingface<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_13">lecture1-introduction.pdf /page_13, doc-id 13</h3>
                    <p style="margin:10px">
                        
                            CPUs vs GPUs<br>
                        
                            How CPU and GPU exist in the same system<br>
                        
                            CPU: Central Processing Unit<br>
                        
                            GPU: Graphic Processing Unit<br>
                        
                            GPUs are highly efficient at executing parallel <br>
                        
                            code<br>
                        
                            Poor choice to execute serial code<br>
                        
                            How consumer grade GPU looks physically<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_14">lecture1-introduction.pdf /page_14, doc-id 14</h3>
                    <p style="margin:10px">
                        
                            Topics covered in the course<br>
                        
                            • We will cover only deep learning techniques (course title is a misnomer) <br>
                        
                            • Foundations of deep learning <br>
                        
                            • Model architectures: Transformers, RNNs, LSTMs, Encoder-decoder <br>
                        
                            • LLM, Instruction fine-tuning <br>
                        
                            • Adapters<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_15">lecture1-introduction.pdf /page_15, doc-id 15</h3>
                    <p style="margin:10px">
                        
                            What will not be covered<br>
                        
                            • Reinforcement learning <br>
                        
                            • CNNs (in-depth) <br>
                        
                            • Probabilistic models (e.g. GMMs, HMMs, Variational learning)<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_16">lecture1-introduction.pdf /page_16, doc-id 16</h3>
                    <p style="margin:10px">
                        
                            Textbooks and references<br>
                        
                            • For PyTorch programming better to follow online videos and Github repositories <br>
                        
                            • Main reference: Speech and Language processing by Jurafsky and Martin (3rd <br>
                        
                            edition) <br>
                        
                            • https://web.stanford.edu/~jurafsky/slp3/ <br>
                        
                            • Other resources: <br>
                        
                            • D2l.ai <br>
                        
                            • Deep learning https://www.deeplearningbook.org/<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_17">lecture1-introduction.pdf /page_17, doc-id 17</h3>
                    <p style="margin:10px">
                        
                            Course Evaluation<br>
                        
                            • 25% exercises <br>
                        
                            • OK to do in groups of 2, but must be declared <br>
                        
                            • Source of materials should be declared (e.g. slide number, text book section, <br>
                        
                            ChatGPT, educated guess) <br>
                        
                            • Tutorials on Tuesday 12h15 — 13h30 (AND-3-02/06) <br>
                        
                            • 75% final exam (20th December NOT 17th December) <br>
                        
                            • Digital exam<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_37">lecture1-introduction.pdf /page_37, doc-id 37</h3>
                    <p style="margin:10px">
                        
                            Tensors<br>
                        
                            A stack of matrices<br>
                        
                            • Multidimensional arrangement of numbers<br>
                        
                            • PyTorch uses tensors as a fundamental representation of data<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_38">lecture1-introduction.pdf /page_38, doc-id 38</h3>
                    <p style="margin:10px">
                        
                            Linear algebra topics covered in the next class<br>
                        
                            • Geometric Matrix-vector product <br>
                        
                            • Eigen values  <br>
                        
                            • Singular Value Decomposition<br>
                        
                            <br>
                        
                    </p>
                </div>
            
        </div>
    
        <div>
        <h2 id="t_1">TOPIC 1: 	x1x2, w1x1, w2x2</h2>
            
                <div>
                    <h3 id="d_18">lecture1-introduction.pdf /page_18, doc-id 18</h3>
                    <p style="margin:10px">
                        
                            Linear Algebra<br>
                        
                            Recap<br>
                        
                            • Algebra of lines <br>
                        
                            • What is a line <br>
                        
                            • In 1 dimension: <br>
                        
                             <br>
                        
                            •<br>
                        
                            : slope <br>
                        
                            •<br>
                        
                             intercept <br>
                        
                            • This is a 1 dimensional line. In 2 dimension we get a plane <br>
                        
                            • In n-dimensions we have a hyperplane<br>
                        
                            y = mx + c<br>
                        
                            m<br>
                        
                            c :<br>
                        
                            x<br>
                        
                            y<br>
                        
                            y = mx + c<br>
                        
                            c<br>
                        
                            θ<br>
                        
                            m = tan θ<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_19">lecture1-introduction.pdf /page_19, doc-id 19</h3>
                    <p style="margin:10px">
                        
                            Hyperplanes<br>
                        
                            • Equation of hyperplane: <br>
                        
                             <br>
                        
                            • Where <br>
                        
                             are vectors in <br>
                        
                            dimensions. This is also stated as <br>
                        
                             <br>
                        
                            • We use bold small letters for vectors, regular font for scalars <br>
                        
                            •<br>
                        
                             is transpose operation (next slide) <br>
                        
                            •<br>
                        
                             is a scalar <br>
                        
                            wTx + b = 0<br>
                        
                            w, x<br>
                        
                            n−<br>
                        
                            x ∈ℝn<br>
                        
                            T<br>
                        
                            b<br>
                        
                            wTx + b = 0<br>
                        
                            wTx + b &gt; 0<br>
                        
                            wTx + b &lt; 0<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_20">lecture1-introduction.pdf /page_20, doc-id 20</h3>
                    <p style="margin:10px">
                        
                            Dot product example<br>
                        
                            • Let us look at a 5-dimensional example<br>
                        
                            x =<br>
                        
                            1.0<br>
                        
                            2.0<br>
                        
                            3.0<br>
                        
                            4.0<br>
                        
                            5.0<br>
                        
                            w =<br>
                        
                            10.0<br>
                        
                            5.0<br>
                        
                            0.0<br>
                        
                            −2.0<br>
                        
                            −2.0<br>
                        
                            wT = [10.0 5.0<br>
                        
                            0.0<br>
                        
                            −2.0<br>
                        
                            −2.0]<br>
                        
                            x =<br>
                        
                            1.0<br>
                        
                            2.0<br>
                        
                            3.0<br>
                        
                            4.0<br>
                        
                            5.0<br>
                        
                            wTx = 10.0 × 1.0 + 2.0 × 5.0 + 3.0 × 0.0 + …<br>
                        
                            wTx = 10.0<br>
                        
                            + 10.0<br>
                        
                            + 0.0<br>
                        
                            + …<br>
                        
                            wTx = −1<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_21">lecture1-introduction.pdf /page_21, doc-id 21</h3>
                    <p style="margin:10px">
                        
                            Geometric interpretation<br>
                        
                            • The part  that goes in the direction of <br>
                        
                             (or vice versa!) i.e. projection of  onto <br>
                        
                             <br>
                        
                            • This is why it can be used to check similarity between two vectors! <br>
                        
                            • Two vectors are orthogonal if their dot product is 0<br>
                        
                            x<br>
                        
                            w<br>
                        
                            x<br>
                        
                            w<br>
                        
                            w<br>
                        
                            x<br>
                        
                            |x|<br>
                        
                            wTx = |w||x|cos θ<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_22">lecture1-introduction.pdf /page_22, doc-id 22</h3>
                    <p style="margin:10px">
                        
                            Vectors (contd.)<br>
                        
                            • Vectors are sequences of scalars <br>
                        
                            • dimensions of a vector = number of vector elements <br>
                        
                            • Convention is to write vector as a column a.k.a column vector<br>
                        
                            w =<br>
                        
                            w1<br>
                        
                            w2<br>
                        
                            w3<br>
                        
                            w4<br>
                        
                            w5<br>
                        
                            wT = [w1<br>
                        
                            w2<br>
                        
                            w3 w4<br>
                        
                            w5]<br>
                        
                            x =<br>
                        
                            x1<br>
                        
                            x2<br>
                        
                            x3<br>
                        
                            x4<br>
                        
                            x5<br>
                        
                            wTx =<br>
                        
                            w1<br>
                        
                            w2<br>
                        
                            w3<br>
                        
                            w4<br>
                        
                            w5<br>
                        
                            ×<br>
                        
                            ×<br>
                        
                            ×<br>
                        
                            ×<br>
                        
                            ×<br>
                        
                            x1<br>
                        
                            x2<br>
                        
                            x3<br>
                        
                            x4<br>
                        
                            x5<br>
                        
                            dim(w) = 5<br>
                        
                            dim(wTx) = 1<br>
                        
                            wTx = w1x1 + w2x2 + w3x3 + w4x4 + w5x5<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_23">lecture1-introduction.pdf /page_23, doc-id 23</h3>
                    <p style="margin:10px">
                        
                            Magnitude and direction of vectors<br>
                        
                            • A vector can be decomposed into two components <br>
                        
                            • Magnitude: length of the vector <br>
                        
                            • Direction of the vector. It has magnitude 1. <br>
                        
                            • Obtained by dividing the vector by its magnitude: a.k.a length normalization<br>
                        
                            w =<br>
                        
                            w1<br>
                        
                            w2<br>
                        
                            w3<br>
                        
                            w4<br>
                        
                            w5<br>
                        
                            |w| =<br>
                        
                            w2<br>
                        
                            1 + w2<br>
                        
                            2 + …w2<br>
                        
                            5<br>
                        
                            ̂w =<br>
                        
                            w1/|w|<br>
                        
                            w2/|w|<br>
                        
                            w3/|w|<br>
                        
                            w4/|w|<br>
                        
                            w5/|w|<br>
                        
                            x<br>
                        
                            y<br>
                        
                            w<br>
                        
                            ̂w<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_24">lecture1-introduction.pdf /page_24, doc-id 24</h3>
                    <p style="margin:10px">
                        
                            Special vectors<br>
                        
                            0 =<br>
                        
                            0<br>
                        
                            0<br>
                        
                            0<br>
                        
                            ⋮<br>
                        
                            1 =<br>
                        
                            1<br>
                        
                            1<br>
                        
                            1<br>
                        
                            ⋮<br>
                        
                            ei =<br>
                        
                            0<br>
                        
                            0<br>
                        
                            0<br>
                        
                            ⋮<br>
                        
                            1<br>
                        
                            ⋮0<br>
                        
                            Zero vector<br>
                        
                            One vector<br>
                        
                            Standard basis vector<br>
                        
                            Foundation for word embeddings<br>
                        
                            e2 =<br>
                        
                            0<br>
                        
                            1<br>
                        
                            0<br>
                        
                            ⋮<br>
                        
                            0<br>
                        
                            e1 =<br>
                        
                            1<br>
                        
                            0<br>
                        
                            0<br>
                        
                            ⋮<br>
                        
                            0<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_25">lecture1-introduction.pdf /page_25, doc-id 25</h3>
                    <p style="margin:10px">
                        
                            Vector operations<br>
                        
                            x =<br>
                        
                            1.0<br>
                        
                            2.0<br>
                        
                            3.0<br>
                        
                            4.0<br>
                        
                            5.0<br>
                        
                            α x =<br>
                        
                            α1.0<br>
                        
                            α2.0<br>
                        
                            α3.0<br>
                        
                            α4.0<br>
                        
                            α5.0<br>
                        
                            α is a scalar<br>
                        
                            y =<br>
                        
                            1.5<br>
                        
                            2.2<br>
                        
                            3.3<br>
                        
                            2.0<br>
                        
                            −5.0<br>
                        
                            x + y =<br>
                        
                            1.0 + 1.5<br>
                        
                            2.0 + 2.2<br>
                        
                            3.0 + 3.3<br>
                        
                            4.0 + 2.0<br>
                        
                            5.0 −5.0<br>
                        
                            =<br>
                        
                            2.5<br>
                        
                            4.2<br>
                        
                            6.3<br>
                        
                            6.0<br>
                        
                            0.0<br>
                        
                            10x =<br>
                        
                            10.0<br>
                        
                            20.0<br>
                        
                            30.0<br>
                        
                            40.0<br>
                        
                            50.0<br>
                        
                            Scalar multiplication<br>
                        
                            Addition of two vectors <br>
                        
                            is element-wise addition<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_26">lecture1-introduction.pdf /page_26, doc-id 26</h3>
                    <p style="margin:10px">
                        
                            Matrices<br>
                        
                            C<br>
                        
                            • Two-dimensional arrangement of values <br>
                        
                            • First dimension is row, number of rows denoted by <br>
                        
                             <br>
                        
                            • Second dimension is column, number of columns denoted by  <br>
                        
                            • We use capital bold letters to denote matrices <br>
                        
                            • Sometimes it is helpful to subscript the shape of the matrix<br>
                        
                            m<br>
                        
                            n<br>
                        
                            • When n=1, we get back column vector <br>
                        
                            • When m=1, we get row vector<br>
                        
                             matrix<br>
                        
                            3 × 3<br>
                        
                            X ∈ℝ3,3<br>
                        
                            Generally we say X ∈ℝm,n<br>
                        
                            X =<br>
                        
                            0.8<br>
                        
                            0.7<br>
                        
                            0.4<br>
                        
                            0.9<br>
                        
                            0.1<br>
                        
                            0.3<br>
                        
                            0.2<br>
                        
                            0.7<br>
                        
                            0.4<br>
                        
                            Columns<br>
                        
                            Rows<br>
                        
                            X =<br>
                        
                            x11<br>
                        
                            x12<br>
                        
                            x13<br>
                        
                            x21 x22<br>
                        
                            x23<br>
                        
                            x31<br>
                        
                            x32<br>
                        
                            x33<br>
                        
                            Matrix with scalar variables<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_27">lecture1-introduction.pdf /page_27, doc-id 27</h3>
                    <p style="margin:10px">
                        
                            Matrix shapes<br>
                        
                            0.8 0.7<br>
                        
                            0.4<br>
                        
                            0.9 0.1<br>
                        
                            0.3<br>
                        
                            0.2 0.7<br>
                        
                            0.4<br>
                        
                            0.8 0.7<br>
                        
                            0.4<br>
                        
                            −0.2<br>
                        
                            −0.1<br>
                        
                            0.9 0.1<br>
                        
                            0.3<br>
                        
                            −0.6<br>
                        
                            0.6<br>
                        
                            0.2 0.7<br>
                        
                            0.4<br>
                        
                            0.9<br>
                        
                            0.2<br>
                        
                            [<br>
                        
                            ]<br>
                        
                            Tall &amp; skinny <br>
                        
                            m ≫n<br>
                        
                            Short &amp; fat m ≪n<br>
                        
                            Square m = n<br>
                        
                            Rectangular m ≠n<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_28">lecture1-introduction.pdf /page_28, doc-id 28</h3>
                    <p style="margin:10px">
                        
                            Matrix Operations<br>
                        
                            Addition of two matrices<br>
                        
                            0.8<br>
                        
                            0.7 0.4<br>
                        
                            0.9<br>
                        
                            0.1 0.3<br>
                        
                            0.2<br>
                        
                            0.7 0.4<br>
                        
                            0.2<br>
                        
                            −0.7<br>
                        
                            0.6<br>
                        
                            0.1<br>
                        
                            0.1<br>
                        
                            0.3<br>
                        
                            −0.2<br>
                        
                            0.3<br>
                        
                            0.4<br>
                        
                            +<br>
                        
                            =<br>
                        
                            0.8 + 0.2 0.7 −0.7 0.4 + 0.6<br>
                        
                            0.9 + 0.1 0.1 + 0.1 0.3 + 0.3<br>
                        
                            0.2 −0.2 0.7 + 0.3 0.4 + 0.4<br>
                        
                            =<br>
                        
                            1.0<br>
                        
                            0.0<br>
                        
                            1.0<br>
                        
                            1.0<br>
                        
                            0.2<br>
                        
                            0.6<br>
                        
                            0.0<br>
                        
                            1.0<br>
                        
                            0.8<br>
                        
                            Both matrices need to be of same dimensions<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_29">lecture1-introduction.pdf /page_29, doc-id 29</h3>
                    <p style="margin:10px">
                        
                            Matrix operations<br>
                        
                            Scalar multiplication<br>
                        
                            2 ×<br>
                        
                            0.8<br>
                        
                            −0.7<br>
                        
                            0.4<br>
                        
                            −0.9<br>
                        
                            0.1<br>
                        
                            0.3<br>
                        
                            0.2<br>
                        
                            0.7<br>
                        
                            0.4<br>
                        
                            =<br>
                        
                            1.6<br>
                        
                            −1.4<br>
                        
                            0.8<br>
                        
                            −1.8<br>
                        
                            0.2<br>
                        
                            0.6<br>
                        
                            0.4<br>
                        
                            1.4<br>
                        
                            0.8<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_30">lecture1-introduction.pdf /page_30, doc-id 30</h3>
                    <p style="margin:10px">
                        
                            Matrix operations<br>
                        
                            Trace and Transpose<br>
                        
                            0.8 0.7<br>
                        
                            0.4<br>
                        
                            0.9 0.1<br>
                        
                            0.3<br>
                        
                            0.2 0.7<br>
                        
                            0.4<br>
                        
                            0.8<br>
                        
                            0.7<br>
                        
                            0.4<br>
                        
                            −0.2<br>
                        
                            −0.1<br>
                        
                            0.9<br>
                        
                            0.1<br>
                        
                            0.3<br>
                        
                            −0.6<br>
                        
                            0.6<br>
                        
                            0.2<br>
                        
                            0.7<br>
                        
                            0.4<br>
                        
                            0.9<br>
                        
                            0.2<br>
                        
                            Trace<br>
                        
                            Sum of diagonal elements<br>
                        
                            0.8 + 0.1 + 0.4<br>
                        
                            0.8 + 0.1 + 0.4<br>
                        
                            tr(A)<br>
                        
                            Transpose AT<br>
                        
                            Swap rows and columns<br>
                        
                            0.8<br>
                        
                            0.9<br>
                        
                            0.2<br>
                        
                            0.7<br>
                        
                            0.1<br>
                        
                            0.7<br>
                        
                            0.4<br>
                        
                            0.3<br>
                        
                            0.4<br>
                        
                            0.8<br>
                        
                            0.9<br>
                        
                            0.2<br>
                        
                            0.7<br>
                        
                            0.1<br>
                        
                            0.7<br>
                        
                            0.4<br>
                        
                            0.3<br>
                        
                            0.4<br>
                        
                            ⋮<br>
                        
                            ⋮<br>
                        
                            ⋮<br>
                        
                            Symmetric matrix: A = AT<br>
                        
                            0.8 0.9<br>
                        
                            0.2<br>
                        
                            0.9 0.1<br>
                        
                            0.7<br>
                        
                            0.2 0.7<br>
                        
                            0.4<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_31">lecture1-introduction.pdf /page_31, doc-id 31</h3>
                    <p style="margin:10px">
                        
                            Linear independence<br>
                        
                            A set of vectors are linearly dependent if they can be scaled and added to obtain <br>
                        
                            0. <br>
                        
                            Otherwise, they are linearly independent  <br>
                        
                            ( 1,0,0 ) , ( 0,1,0 ) and ( 0,0,1 ) linearly independent. <br>
                        
                            ( 1,0,1 ) , ( 2,-1,1 ) , ( 3,-1,2 ) are linearly dependent.<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_32">lecture1-introduction.pdf /page_32, doc-id 32</h3>
                    <p style="margin:10px">
                        
                            Matrix rank<br>
                        
                            • Consider the row (or column) vectors in a matrix <br>
                        
                            • The rank of the matrix is the size of the largest set of linearly independent <br>
                        
                            vectors<br>
                        
                            X =<br>
                        
                            1<br>
                        
                            2<br>
                        
                            3<br>
                        
                            0<br>
                        
                            2<br>
                        
                            2<br>
                        
                            1<br>
                        
                            4<br>
                        
                            5<br>
                        
                            • Rank of the above matrix is 2 <br>
                        
                            • Only row 1 and 2 are linearly independent <br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_33">lecture1-introduction.pdf /page_33, doc-id 33</h3>
                    <p style="margin:10px">
                        
                            Matrix multiplication <br>
                        
                            • Let <br>
                        
                             be a <br>
                        
                             matrix <br>
                        
                            • Let <br>
                        
                             be <br>
                        
                             matrix <br>
                        
                            •<br>
                        
                             can be multiplied with <br>
                        
                             only if <br>
                        
                             <br>
                        
                            • The resulting matrix <br>
                        
                             is <br>
                        
                            X1<br>
                        
                            m × n<br>
                        
                            X2<br>
                        
                            a × b<br>
                        
                            X1<br>
                        
                            X2<br>
                        
                            a = n<br>
                        
                            X1X2<br>
                        
                            m × b<br>
                        
                            X1 = [<br>
                        
                            2 4<br>
                        
                            3 2<br>
                        
                            1 2]<br>
                        
                            X2 = [<br>
                        
                            3<br>
                        
                            2<br>
                        
                            1<br>
                        
                            1]<br>
                        
                            X1X2 = [<br>
                        
                            2<br>
                        
                            4<br>
                        
                            3<br>
                        
                            2<br>
                        
                            1<br>
                        
                            2] [<br>
                        
                            3<br>
                        
                            2<br>
                        
                            1<br>
                        
                            1] =<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_34">lecture1-introduction.pdf /page_34, doc-id 34</h3>
                    <p style="margin:10px">
                        
                            Special matrices<br>
                        
                            D = [<br>
                        
                            0.8 0.0<br>
                        
                            0.0<br>
                        
                            0.0 0.1<br>
                        
                            0.0<br>
                        
                            0.0 0.0<br>
                        
                            0.4]<br>
                        
                            I3 = [<br>
                        
                            1<br>
                        
                            0 0<br>
                        
                            0<br>
                        
                            1 0<br>
                        
                            0<br>
                        
                            0 1]<br>
                        
                            In =<br>
                        
                            1<br>
                        
                            0<br>
                        
                            0<br>
                        
                            …<br>
                        
                            0<br>
                        
                            0<br>
                        
                            1<br>
                        
                            0<br>
                        
                            …<br>
                        
                            0<br>
                        
                            0<br>
                        
                            0<br>
                        
                            1<br>
                        
                            …<br>
                        
                            0<br>
                        
                            ⋮<br>
                        
                            ⋮<br>
                        
                            ⋮<br>
                        
                            …<br>
                        
                            0<br>
                        
                            0<br>
                        
                            ⋮<br>
                        
                            ⋮<br>
                        
                            …<br>
                        
                            ?<br>
                        
                            Diagonal matrix<br>
                        
                            Identity matrix<br>
                        
                             identity matrix<br>
                        
                            3 × 3<br>
                        
                             identity matrix<br>
                        
                            n × n<br>
                        
                            Any matrix multiplied with  is the matrix itself<br>
                        
                            I<br>
                        
                            AI = IA = A<br>
                        
                            When a matrix is pre-<br>
                        
                            multiplied by a diagonal <br>
                        
                            matrix, i.e <br>
                        
                             it scales <br>
                        
                            the rows<br>
                        
                            DX<br>
                        
                            When it is post-<br>
                        
                            multiplied, that is <br>
                        
                             it <br>
                        
                            scales the columns<br>
                        
                            XD<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_35">lecture1-introduction.pdf /page_35, doc-id 35</h3>
                    <p style="margin:10px">
                        
                            Matrix identities<br>
                        
                            • Distributive law <br>
                        
                             <br>
                        
                            • Associative law <br>
                        
                             <br>
                        
                            • Transpose after product: <br>
                        
                            A(B + C) = AB + AC<br>
                        
                            A + (B + C) = (A + B) + C<br>
                        
                            (AB)T = BTAT<br>
                        
                            For more fun with matrix identities see “Matrix cookbook”<br>
                        
                            <br>
                        
                    </p>
                </div>
            
                <div>
                    <h3 id="d_36">lecture1-introduction.pdf /page_36, doc-id 36</h3>
                    <p style="margin:10px">
                        
                            Matrix inverse<br>
                        
                            • Only square matrices can be inverted <br>
                        
                            • Only full-rank matrices can be inverted<br>
                        
                            AA−1 = A−1A = I<br>
                        
                            <br>
                        
                    </p>
                </div>
            
        </div>
    
    </div>
    </body>
    </html>
    